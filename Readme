## Abstract

>Panoptic Scene Graph Generation (PSG) combines panoptic segmentation and scene graph generation to provide a holistic understanding of visual scenes. With the remarkable advances in large multimodal models (LMMs), recent PSG research has increasingly focused on generating open-world panoptic scene graphs in the wild.
However, current open-world PSG models typically suffer from three limitations: i) limited object and predicate categories for the open-world PSG training and evaluation; ii) difficulty in predicting spatially-related predicates; and iii) insufficient exploration of relatedness contexts. To address these limitations, we first curate a new dataset OW-PSG that is specifically designed for open-set PSG evaluation. We then propose the Relation Diffusion RelDiff model for open-world Panoptic Scene Graph Generation, which combines a diffusion model and a large multimodal model (LMM) by fine-tuning a denoising network with a lightweight LoRA technique. Moreover, we develop a Relation Graph Transformer to capture comprehensive contextual information built upon a  spatial and semantic graph. Last, we evaluate our model on the benchmark PSG, VG,  and our proposed OW-PSG dataset. Across the board, RelDiff demonstrates RelDiff's superior generalization capabilities and achieves state-of-the-art performance in the open-world setting.
